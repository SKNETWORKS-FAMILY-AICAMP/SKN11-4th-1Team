{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ff719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ \n",
    "file_paths = {\n",
    "    \"term\": \"./metadata/term.json\",\n",
    "    \"load_traffic_law\": \"./metadata/load_traffic_law.json\",\n",
    "    \"modifier\": \"./metadata/modifier.json\",\n",
    "    \"car_case\": \"./metadata/car_to_car.json\",\n",
    "    \"precedent\": \"./metadata/precedent.json\"\n",
    "}\n",
    "\n",
    "# êµí†µì‚¬ê³  ì¼€ì´ìŠ¤ìš© í•„ë“œ ìƒìˆ˜\n",
    "CASE_ID = \"ì‚¬ê±´ ID\"\n",
    "CASE_TITLE = \"ì‚¬ê±´ ì œëª©\"\n",
    "CASE_SITUATION = \"ì‚¬ê³ ìƒí™©\"\n",
    "BASE_RATIO = \"ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨\"\n",
    "MODIFIERS = \"ì¼€ì´ìŠ¤ë³„ ê³¼ì‹¤ë¹„ìœ¨ ì¡°ì •ì˜ˆì‹œ\"\n",
    "LAW_REFERENCES = \"ê´€ë ¨ ë²•ê·œ\"\n",
    "PRECEDENT = \"ì°¸ê³  íŒë¡€\"\n",
    "REASON = \"ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨ í•´ì„¤\"\n",
    "\n",
    "# JSON ë¡œë“œ í•¨ìˆ˜\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸í˜• JSON ë³€í™˜ (term, modifier, law_meta)\n",
    "def convert_list_to_documents(data_list, doc_type):\n",
    "    return [\n",
    "        Document(page_content=json.dumps(item, ensure_ascii=False), metadata={\"type\": doc_type})\n",
    "        for item in data_list\n",
    "    ]\n",
    "\n",
    "def convert_car_case_documents(data_list):\n",
    "    documents = []\n",
    "\n",
    "    def safe_value(value):\n",
    "        if isinstance(value, list):\n",
    "            return \", \".join(map(str, value))\n",
    "        elif isinstance(value, dict):\n",
    "            return json.dumps(value, ensure_ascii=False)\n",
    "        elif value is None:\n",
    "            return \"\"  # nullë„ í—ˆìš© ì•ˆ ë˜ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬\n",
    "        else:\n",
    "            return str(value)\n",
    "\n",
    "    for item in data_list:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "\n",
    "        # page_contentëŠ” ì›ë³¸ ì „ì²´ JSON ë¬¸ìì—´\n",
    "        content = json.dumps(item, ensure_ascii=False)\n",
    "\n",
    "        # ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨ í•´ì„¤ì´ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ â†’ ë¬¸ìì—´ë¡œ ë³‘í•©\n",
    "        reason = item.get(REASON)\n",
    "        if isinstance(reason, list):\n",
    "            reason = \"\\n\".join(map(str, reason))\n",
    "\n",
    "        metadata = {\n",
    "            \"type\": \"car_case\",\n",
    "            \"id\": safe_value(item.get(CASE_ID)),\n",
    "            \"title\": safe_value(item.get(CASE_TITLE)),\n",
    "            \"situation\": safe_value(item.get(CASE_SITUATION)),\n",
    "            \"base_ratio\": safe_value(item.get(BASE_RATIO)),\n",
    "            \"modifiers\": safe_value(item.get(MODIFIERS)),\n",
    "            \"load_traffic_law\": safe_value(item.get(LAW_REFERENCES)),\n",
    "            \"precedent\": safe_value(item.get(PRECEDENT)),\n",
    "            \"reason\": safe_value(reason)\n",
    "        }\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "# ë„ë¡œêµí†µë²• law JSON â†’ ë¬¸ì„œí™”\n",
    "def convert_law_json_to_documents(data_dict):\n",
    "    documents = []\n",
    "\n",
    "    def normalize(item):\n",
    "        return json.dumps(item, ensure_ascii=False) if isinstance(item, dict) else str(item)\n",
    "\n",
    "    for law_name, content in data_dict.items():\n",
    "        if isinstance(content, dict):\n",
    "            for clause, text in content.items():\n",
    "                lines = [normalize(x) for x in (text if isinstance(text, list) else [text])]\n",
    "                full_text = f\"{law_name} {clause}\\n\" + \"\\n\".join(lines)\n",
    "                documents.append(Document(page_content=full_text, metadata={\"type\": \"load_traffic_law\"}))\n",
    "        else:\n",
    "            lines = [normalize(x) for x in (content if isinstance(content, list) else [content])]\n",
    "            full_text = f\"{law_name}\\n\" + \"\\n\".join(lines)\n",
    "            documents.append(Document(page_content=full_text, metadata={\"type\": \"load_traffic_law\"}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# ë¬¸ì„œí™” ì‹¤í–‰\n",
    "term_docs = convert_list_to_documents(load_json(file_paths[\"term\"]), \"term\")\n",
    "modifier_docs = convert_list_to_documents(load_json(file_paths[\"modifier\"]), \"modifier\")\n",
    "precedent_docs = convert_list_to_documents(load_json(file_paths[\"precedent\"]), \"precedent\")\n",
    "car_case_docs = convert_car_case_documents(load_json(file_paths[\"car_case\"]))\n",
    "load_traffic_law_docs = convert_law_json_to_documents(load_json(file_paths[\"load_traffic_law\"]))\n",
    "\n",
    "\n",
    "# ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "all_docs = term_docs + modifier_docs + car_case_docs + precedent_docs + load_traffic_law_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma  # persist ì§€ì›\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. ì²­í¬ í¬ê¸° ì¡°ì • (500~1000 ê¶Œì¥)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\\\. )\", \" \", \"\"],\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "# 2. ë¬¸ì„œ ë¶„í• \n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# 4. Chroma DBì— ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì €ì¥\n",
    "batch_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits[:batch_size],  # ì²« ë°°ì¹˜\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./vectordb\"\n",
    ")\n",
    "\n",
    "# ë‚¨ì€ ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€ \n",
    "for i in range(0, len(all_splits), batch_size):\n",
    "    try:\n",
    "        batch = all_splits[i:i+batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "        vectorstore.persist()  # ë§¤ ë°°ì¹˜ í›„ ì¦‰ì‹œ ì €ì¥\n",
    "    except Exception as e:\n",
    "        print(f\"ë°°ì¹˜ {i}~{i+batch_size} ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aaa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1. LLM ì„¤ì •\n",
    "model = ChatOpenAI(model='gpt-4o', temperature=0.5)\n",
    "\n",
    "# 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "ë‹¤ìŒ 'ì‚¬ê³  ìƒí™© ì„¤ëª…'ì— ëŒ€í•´ 'ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ê³¼ì‹¤ ë¹„ìœ¨ ë° ë²•ì  íŒë‹¨ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "ì‚¬ê³  ìƒí™©: {question}\n",
    "\n",
    "ë¬¸ì„œ ë‚´ìš©: {context}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹:\n",
    "1. ê³¼ì‹¤ë¹„ìœ¨: Aì°¨ëŸ‰ xx% vs Bì°¨ëŸ‰ xx%\n",
    "2. ì‚¬ê³  ì›ì¸ ë° íŒë‹¨ ê·¼ê±°:\n",
    "   - [ì‚¬ê³ ì˜ ì „ê°œ, ê° ì°¨ëŸ‰ì˜ í–‰ìœ„, ê³¼ì‹¤ ìš”ì†Œ ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]\n",
    "3. ê´€ë ¨ ë²•ë¥  ì¡°í•­:\n",
    "   - [ì˜ˆ: ë„ë¡œêµí†µë²• ì œ10ì¡° ì œ2í•­, ì œ27ì¡° ì œ1í•­ ë“±]\n",
    "4. ì°¸ê³  íŒë¡€:\n",
    "   - [ì˜ˆ: ëŒ€ë²•ì› 2023ë‹¤12345, ì„œìš¸ì¤‘ì•™ì§€ë²• 2022ê°€ë‹¨123456 ë“±]\n",
    "\n",
    "ì¡°ê±´:\n",
    "- ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ë§Œ ì°¸ê³ í•´ íŒë‹¨í•˜ì„¸ìš”.\n",
    "- ë²•ë¥  ì¡°í•­ê³¼ íŒë¡€ê°€ ëª…ì‹œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìœ ì‚¬í•˜ê±°ë‚˜ ì¶”ì • ê°€ëŠ¥í•œ ê·¼ê±°ë¥¼ ì œì‹œí•´ë„ ë©ë‹ˆë‹¤.\n",
    "- ì „ì²´ ë‹µë³€ì€ í¬ë§·ì— ë§ê²Œ ê°„ê²°í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# 3. ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 4. QA ì²´ì¸ ì¬êµ¬ì„± (ìˆ˜ì • ë¶€ë¶„)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 5. ì‹¤ì œ ì§ˆì˜ ì‹¤í–‰\n",
    "query = \"Bì°¨ëŸ‰ì´ ë¹„ë³´í˜¸ì¢ŒíšŒì „ í‘œì§€ê°€ ì—†ëŠ” êµì°¨ë¡œì—ì„œ ë…¹ìƒ‰ë“±(ì¢ŒíšŒì „ í™”ì‚´í‘œ ì—†ìŒ)ì— ì¢ŒíšŒì „ ì‹œë„, ì§ì§„í•˜ëŠ” Aì°¨ëŸ‰ê³¼ ì¶©ëŒí•œ ì‚¬ê³ ê°€ ë°œìƒí–ˆì–´. ê³¼ì‹¤ë¹„ìœ¨ì´ ì–´ë–»ê²Œ ë¼??\"\n",
    "\n",
    "res = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# 6. ì¶œë ¥\n",
    "print(\"âœ… ë‹µë³€:\\n\", res[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ìš©ì–´ë‚˜ ë²•ë¥  ì¡°í•­, íŒë¡€ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë¬¸ì„œ: {context}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹:\n",
    "- ìš©ì–´/ì¡°í•­ ì •ì˜: [ì •í™•í•œ ì„¤ëª…]\n",
    "- ì¶œì²˜ê°€ ëª…ì‹œëœ ê²½ìš°: ê´€ë ¨ ë²•ë¥ /ì¡°ë¬¸ ë²ˆí˜¸/íŒë¡€ëª…ì„ ë°˜ë“œì‹œ í¬í•¨\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "detail_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": detail_prompt}\n",
    ")\n",
    "\n",
    "# runì€ question í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤í–‰ë¨!\n",
    "res2 = detail_chain.run(\"ë³´í–‰ììš°ì„ ë„ë¡œê°€ ë­”ì§€ ì„¤ëª…í•´ì¤˜\")\n",
    "\n",
    "print(\"âœ… ê¸°ëŠ¥â‘¡ ë‹µë³€:\\n\", res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "model = ChatOpenAI(model='gpt-4o', temperature=0.3)\n",
    "\n",
    "# ê¸°ëŠ¥ 1ë²ˆ. ì‚¬ê³  ìƒí™© íŒë‹¨ í”„ë¡¬í”„íŠ¸\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "ë„ˆëŠ” êµí†µì‚¬ê³  ê³¼ì‹¤ íŒë‹¨ ì „ë¬¸ê°€ì•¼.\n",
    "ë‹¤ìŒ 'ì‚¬ê³  ìƒí™©'ì„ ë³´ê³ , ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì‹¤ë¹„ìœ¨ì„ ê³„ì‚°í•´ì¤˜.\n",
    "\n",
    "ì‚¬ê³  ìƒí™©:\n",
    "{question}\n",
    "\n",
    "ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹:\n",
    "1. ê³¼ì‹¤ë¹„ìœ¨: Aì°¨ëŸ‰ xx% vs Bì°¨ëŸ‰ xx%\n",
    "2. íŒë‹¨ ê·¼ê±° ìš”ì•½\n",
    "3. ì ìš© ë²•ë¥ \n",
    "4. ì°¸ê³  íŒë¡€\n",
    "\n",
    "ì¡°ê±´:\n",
    "- ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•´\n",
    "- ë¬¸ì„œì— ì—†ë‹¤ë©´ íŒë‹¨ ë³´ë¥˜í•˜ê±°ë‚˜ ìœ ì‚¬ ì‚¬ë¡€ë¡œ ê·¼ê±° ì„¤ëª…í•´\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ê¸°ëŠ¥ 2ë²ˆ. ìš©ì–´/ë²•ë¥  ì„¤ëª… í”„ë¡¬í”„íŠ¸\n",
    "detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ìš©ì–´ë‚˜ ë²•ë¥  ì¡°í•­, íŒë¡€ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë¬¸ì„œ: {context}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹:\n",
    "- ìš©ì–´/ì¡°í•­ ì •ì˜: [ì •í™•í•œ ì„¤ëª…]\n",
    "- ì¶œì²˜ê°€ ëª…ì‹œëœ ê²½ìš°: ê´€ë ¨ ë²•ë¥ /ì¡°ë¬¸ ë²ˆí˜¸/íŒë¡€ëª…ì„ ë°˜ë“œì‹œ í¬í•¨\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# ì‚¬ê³  ìƒí™© íŒë‹¨ ì²´ì¸\n",
    "accident_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# ì„¤ëª…ìš© ì²´ì¸\n",
    "detail_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": detail_prompt}\n",
    ")\n",
    "\n",
    "def classify_query(user_input: str) -> str:\n",
    "    accident_keywords = [\"ì¶©ëŒ\", \"ì‚¬ê³ \", \"ê³¼ì‹¤\", \"Aì°¨ëŸ‰\", \"Bì°¨ëŸ‰\", \"ì‹ í˜¸ìœ„ë°˜\", \"ë¶€ë”ªí˜\", \"ì§„ì…\", \"êµì°¨ë¡œ\"]\n",
    "    explanation_keywords = [\"ë¬´ì—‡\", \"ë­ì•¼\", \"ì •ì˜\", \"ì„¤ëª…\", \"íŒë¡€\", \"ì¡°í•­\", \"ë²•ì›\", \"ë²•\", \"ë„ë¡œêµí†µë²•\" ,\"ë¬´ìŠ¨ ëœ»\", \"ì˜ë¯¸\", \"ìš©ì–´\", \"ì´ë€\"]\n",
    "\n",
    "    # ê¸°ëŠ¥â‘¡ ì„¤ëª… ìš°ì„  ì²˜ë¦¬\n",
    "    if any(word in user_input for word in explanation_keywords):\n",
    "        return \"detail\"\n",
    "    if any(word in user_input for word in accident_keywords):\n",
    "        return \"accident\"\n",
    "    return \"detail\"\n",
    "\n",
    "\n",
    "# í†µí•© ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_query(user_input: str):\n",
    "    category = classify_query(user_input)\n",
    "    if category == \"accident\":\n",
    "        result = accident_chain.run(user_input)\n",
    "        print(\"âœ… ì‚¬ê³  ìƒí™© íŒë‹¨ ê²°ê³¼:\\n\", result)\n",
    "    else:\n",
    "        result = detail_chain.run(user_input)\n",
    "        print(\"ğŸ“˜ ìš©ì–´/ë²•ë¥  ì„¤ëª… ê²°ê³¼:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292e79b",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5532e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\"Aì°¨ëŸ‰ì´ ì ìƒ‰ì‹ í˜¸ì— ì§ì§„ ì¤‘ì´ê³ , Bì°¨ëŸ‰ì€ ì¢ŒíšŒì „ ì‹ í˜¸ ì—†ì´ ë…¹ìƒ‰ë“±ì—ì„œ ì¢ŒíšŒì „ ì¤‘ ì¶©ëŒí•œ ì‚¬ê³ ì•¼. ê³¼ì‹¤ë¹„ìœ¨ì€?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\"ë¹„ë³´í˜¸ ì¢ŒíšŒì „ì´ ë­”ì§€ ì„¤ëª…í•´ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9689ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\"ëŒ€ë²•ì› 92ë„2077 íŒê²°ë¬¸ ë‚´ìš©ì„ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ee5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\"ì„œìš¸ê³ ë“±ë²•ì› 2002ë‚˜57692 íŒê²°ë¬¸ ë‚´ìš©ì„ ì•Œë ¤ì¤˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
