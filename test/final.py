#!/usr/bin/env python
# coding: utf-8

# # Variable

# In[ ]:


# íŒŒì¼ ê²½ë¡œ
FILE_PATH = {
    'TERM' : '../metadata/term.json',                            # ìš©ì–´
    'LOAD_TRAFFIC_LAW' : '../metadata/load_traffic_law.json',    # ë„ë¡œêµí†µë²•
    'MODIFIER' : '../metadata/modifier.json',                    # ìˆ˜ì •ìš”ì†Œ
    'CAR_CASE' : '../metadata/car_to_car.json',                  # ì°¨ sì°¨ ì‚¬ê³  ì¼€ì´ìŠ¤
    'PRECEDENT' : '../metadata/precedent.json',                  # ì°¸ê³  íŒë¡€

    'VECTOR_DB' : '../vector_db',                                # ë²¡í„° DB ì €ì¥ê²½ë¡œ
}

# ë²¡í„°DB ì»¬ë ‰ì…˜ ì´ë¦„ ì •ì˜
VECTOR_DB_COLLECTION = {
    'TERM' : "term",
    'LOAD_TRAFFIC_LAW' : "load_traffic_law",
    'MODIFIER' : "modifier",
    'CAR_CASE' : "car_case",
    'PRECEDENT' : "precedent",
}

# JSON íŒŒì¼ KEY ê°’ ì •ì˜
METADATA_KEY = {
    'ACCIDENT_CASE' : {
        'CASE_ID' : "ì‚¬ê±´ ID",
        'CASE_TITLE' : "ì‚¬ê±´ ì œëª©",
        'CASE_SITUATION' : "ì‚¬ê³ ìƒí™©",
        'BASE_RATIO' : "ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨",
        'MODIFIERS' : "ì¼€ì´ìŠ¤ë³„ ê³¼ì‹¤ë¹„ìœ¨ ì¡°ì •ì˜ˆì‹œ",
        'LAW_REFERENCES' : "ê´€ë ¨ ë²•ê·œ",
        'PRECEDENT' : "ì°¸ê³  íŒë¡€",
        'REASON' : "ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨ í•´ì„¤",
    },

    'PRECEDENT' : {
        'COURT' : "court",
        'CASE_ID' : "case_id",
        'CONTENT' : "content",
    },

    'TERM' : {
        'TERM' : "term",
        'DESC' : "desc",
    }
}


# # 1. Documentation (ë¬¸ì„œí™”)

# ### IMPORT

# In[ ]:


from langchain.schema import Document
import json


# ### Function

# In[ ]:


# JSON ë¡œë“œ í•¨ìˆ˜
def load_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)



# ë¦¬ìŠ¤íŠ¸í˜• JSON -> Document ë³€í™˜ (modifier)
def convert_list_to_documents(data_list, doc_type):
    return [
        Document(page_content=json.dumps(item, ensure_ascii=False), metadata={"type": doc_type})
        for item in data_list
    ]



# precedent JSON -> Document ë³€í™˜
def convert_precedent_to_docs(data_list):
    return [
        Document(
            page_content=f"{item[METADATA_KEY['PRECEDENT']['COURT']]} {item[METADATA_KEY['PRECEDENT']['CASE_ID']]} : {item[METADATA_KEY['PRECEDENT']['CONTENT']]}",
            metadata={
                METADATA_KEY['PRECEDENT']['COURT']: item[METADATA_KEY['PRECEDENT']['COURT']],
                METADATA_KEY['PRECEDENT']['CASE_ID']: item[METADATA_KEY['PRECEDENT']['CASE_ID']],
            }
        ) for item in data_list
    ]



# term JSON -> Document ë³€í™˜
def convert_term_to_docs(data_list):
    return [
        Document(
            page_content=f"{item[METADATA_KEY['TERM']['TERM']]} : {item[METADATA_KEY['TERM']['DESC']]}",
            metadata={
                METADATA_KEY['TERM']['TERM']: item[METADATA_KEY['TERM']['TERM']]
            }
        ) for item in data_list
    ]



# car_case JSON -> Document ë³€í™˜
def convert_car_case_to_docs(data_list):
    documents = []

    def safe_value(value):
        if isinstance(value, list):
            return ", ".join(map(str, value))
        elif isinstance(value, dict):
            return json.dumps(value, ensure_ascii=False)
        elif value is None:
            return ""  # nullë„ í—ˆìš© ì•ˆ ë˜ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        else:
            return str(value)

    for item in data_list:
        if not isinstance(item, dict):
            continue

        # page_contentëŠ” ì›ë³¸ ì „ì²´ JSON ë¬¸ìì—´
        content = json.dumps(item, ensure_ascii=False)

        # ê¸°ë³¸ ê³¼ì‹¤ë¹„ìœ¨ í•´ì„¤ì´ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ â†’ ë¬¸ìì—´ë¡œ ë³‘í•©
        reason = item.get(METADATA_KEY['ACCIDENT_CASE']['REASON'])
        if isinstance(reason, list):
            reason = "\n".join(map(str, reason))

        metadata = {
            "type": "car_case",
            "id": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['CASE_ID'])),
            "title": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['CASE_TITLE'])),
            "situation": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['CASE_SITUATION'])),
            "base_ratio": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['BASE_RATIO'])),
            "modifiers": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['MODIFIERS'])),
            "load_traffic_law": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['LAW_REFERENCES'])),
            "precedent": safe_value(item.get(METADATA_KEY['ACCIDENT_CASE']['PRECEDENT'])),
            "reason": safe_value(reason)
        }

        documents.append(Document(page_content=content, metadata=metadata))

    return documents



# ë„ë¡œêµí†µë²• law JSON â†’ ë¬¸ì„œí™”
def convert_traffic_law_to_docs(data_dict):
    documents = []

    # def normalize(item):
    #     return json.dumps(item, ensure_ascii=False) if isinstance(item, dict) else str(item)

    # for law_name, content in data_dict.items():
    #     if isinstance(content, dict):
    #         for clause, text in content.items():
    #             lines = [normalize(x) for x in (text if isinstance(text, list) else [text])]
    #             full_text = f"{law_name} {clause}\n" + "\n".join(lines)
    #             documents.append(Document(page_content=full_text, metadata={"type": "load_traffic_law"}))
    #     else:
    #         lines = [normalize(x) for x in (content if isinstance(content, list) else [content])]
    #         full_text = f"{law_name}\n" + "\n".join(lines)
    #         documents.append(Document(page_content=full_text, metadata={"type": "load_traffic_law"}))
    
    # return documents
    for article_title, article_content in data_dict.items():
        main_clause = article_title.split("(")[0].strip()
        clause_name = article_title.split("(")[1].replace(")", "").strip()
        for sub_clause, texts in article_content.items():
            clause_num = int(sub_clause.replace("í•­", ""))
            content = " ".join(texts)
            metadata = {
                "ë²•ë¥ ì¡°ë¬¸": main_clause,
                "ì¡°í•­ëª…": clause_name,
                "í•­ë²ˆí˜¸": clause_num,
                "ì „ì²´ì°¸ì¡°": f"{main_clause} {sub_clause}"
            }
            documents.append(Document(page_content=content, metadata=metadata))
    
    return documents




# ì „ì²´ JSON ë¬¸ì„œí™”
# def convert_all_docs():
#     term_docs = convert_term_to_docs(load_json(FILE_PATH['TERM']))
#     modifier_docs = convert_list_to_documents(load_json(FILE_PATH['MODIFIER']), 'modifier')
#     precedent_docs = convert_precedent_to_docs(load_json(FILE_PATH['PRECEDENT']))
#     car_case_docs = convert_car_case_to_docs(load_json(FILE_PATH['CAR_CASE']))
#     load_traffic_law_docs = convert_load_traffic_law_to_docs(load_json(FILE_PATH['LOAD_TRAFFIC_LAW']))

#     return term_docs + modifier_docs + precedent_docs + car_case_docs + load_traffic_law_docs


# ### Make Document

# In[ ]:


# ì „ì²´ ë¬¸ì„œí™”
# all_docs = convert_all_docs()

term_docs = convert_term_to_docs(load_json(FILE_PATH['TERM']))
precedent_docs = convert_precedent_to_docs(load_json(FILE_PATH['PRECEDENT']))
load_traffic_law_docs = convert_traffic_law_to_docs(load_json(FILE_PATH['LOAD_TRAFFIC_LAW']))

car_case_docs = convert_car_case_to_docs(load_json(FILE_PATH['CAR_CASE']))
modifier_docs = convert_list_to_documents(load_json(FILE_PATH['MODIFIER']), 'modifier')
accident_docs = car_case_docs + modifier_docs


# # 2. Vector DB ì €ì¥

# ### IMPORT

# In[ ]:


from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings


# ### Embedding Model

# In[ ]:


# ì„ë² ë”© ëª¨ë¸
embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')


# ### ê° ë¬¸ì„œë³„ Collection ë‚˜ëˆ  ì €ì¥

# In[ ]:


# Document -> Vector DB ì €ì¥
def docs_to_chroma_db(docs, collection_name):
    db = Chroma.from_documents(
        documents=docs,
        embedding=embedding_model,
        persist_directory=FILE_PATH['VECTOR_DB'],
        collection_name=collection_name
    )
    return db


# In[ ]:


# ChromaDBì— ì €ì¥
term_db = docs_to_chroma_db(term_docs, VECTOR_DB_COLLECTION['TERM'])
precedent_db = docs_to_chroma_db(precedent_docs, VECTOR_DB_COLLECTION['PRECEDENT'])
load_traffic_law_db = docs_to_chroma_db(load_traffic_law_docs, VECTOR_DB_COLLECTION['LOAD_TRAFFIC_LAW'])

car_case_db = docs_to_chroma_db(car_case_docs + modifier_docs, VECTOR_DB_COLLECTION['CAR_CASE'])


# In[ ]:


# from langchain_text_splitters import RecursiveCharacterTextSplitter

# # 1. ì²­í¬ í¬ê¸° ì¡°ì • (500~1000 ê¶Œì¥)
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=500,
#     chunk_overlap=100,
#     length_function=len,
#     separators=["\n\n", "\n", "(?<=\\. )", " ", ""],
#     is_separator_regex=True,
# )

# # 2. ë¬¸ì„œ ë¶„í• 
# all_splits = text_splitter.split_documents(all_docs)

# # 3. ì„ë² ë”© ëª¨ë¸ ()
# embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')

# # 4. Chroma DBì— ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì €ì¥
# batch_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜
# vectorstore = Chroma.from_documents(
#     documents=all_splits[:batch_size],  # ì²« ë°°ì¹˜
#     embedding=embedding_model,
#     persist_directory=FILE_PATH['VECTOR_DB']
# )

# # ë‚¨ì€ ì²­í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€ 
# for i in range(0, len(all_splits), batch_size):
#     try:
#         batch = all_splits[i:i+batch_size]
#         vectorstore.add_documents(batch)
#         vectorstore.persist()  # ë§¤ ë°°ì¹˜ í›„ ì¦‰ì‹œ ì €ì¥
#     except Exception as e:
#         print(f"ë°°ì¹˜ {i}~{i+batch_size} ì €ì¥ ì‹¤íŒ¨: {e}")

# vectorstore.persist()


# # ì‚¬ìš©ì ëª©ì ì— ë”°ë¥¸ LLM ìˆ˜í–‰ë™ì‘ ë¶„ë¦¬

# ### IMPORT

# In[ ]:


# ê¸°ëŠ¥ ë¶„ë¥˜ ë° ë¼ìš°íŒ… ì²˜ë¦¬ + ì‚¬ê³  ìƒí™© ê¸°ë°˜ ê³¼ì‹¤ë¹„ìœ¨ íŒë‹¨ í¬í•¨
import re
import numpy as np
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer
from langchain.chains import LLMChain

from langchain.chains import RetrievalQA
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers import SelfQueryRetriever


# In[ ]:


# ì‚¬ìš©ì ì§ˆì˜ ëª©ì  ì •ì˜
SITUATION_CASE = {
    'GENERAL' : "general",

    'ACCIDENT' : "accident",
    'TERM' : "term",
    'PRECEDENT' : "precedent",
    'LAW' : "law",
}

GPT_4O_MODEL = ChatOpenAI(model="gpt-4o", temperature=0)
GPT_3_5_MODEL = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)


# ### Function

# In[ ]:


# ì§ˆì˜ ëª©ì  êµ¬ë¶„
def classify_query(user_input: str) -> str:
    classification_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” êµí†µì‚¬ê³  ìƒë‹´ ì±—ë´‡ì˜ ì§ˆë¬¸ ë¶„ë¥˜ê¸°ì•¼.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ìŒ ì¤‘ ì–´ë–¤ ìœ í˜•ì¸ì§€ íŒë‹¨í•´:

1. ì‚¬ê³  ìƒí™© íŒë‹¨ ì§ˆë¬¸ (ì˜ˆ: ì‚¬ê³  ê²½ìœ„ ì„¤ëª…, ê³¼ì‹¤ë¹„ìœ¨ ìš”ì²­)
2. ë„ë¡œêµí†µë²•ë¥  ì„¤ëª… ì§ˆë¬¸ (ì˜ˆ: ë„ë¡œêµí†µë²• ì¡°í•­ ì •ì˜, ë„ë¡œêµí†µë²• ì œ5ì¡° 1í•­ ë“±)
3. íŒë¡€ ì„¤ëª… ì§ˆë¬¸ (ì˜ˆ: íŒë¡€ ë²ˆí˜¸, ì„œìš¸ê³ ë“±ë²•ì› 2002ë‚˜57692 ë“±)
4. êµí†µì‚¬ê³  ìš©ì–´ ì„¤ëª… ì§ˆë¬¸ (ì˜ˆ: ìš©ì–´, 'ê³¼ì‹¤' ì •ì˜, 'íšŒì „êµì°¨ë¡œ' ì •ì˜ ë“±)
5. ì¼ë°˜ ì§ˆë¬¸ (ì˜ˆ: "ë„ˆëŠ” ëˆ„êµ¬ì•¼?", "ë‚ ì”¨ ì–´ë•Œ?", "GPTë€ ë­ì•¼?" ë“± êµí†µì‚¬ê³ ì™€ ë¬´ê´€í•œ ì§ˆë¬¸)

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ í•´:
- accident
- precedent
- law
- term
- general

ë‹¤ë¥¸ ë§ ì—†ì´ ìœ„ ë‹¤ì„¯ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ì¶œë ¥í•´.

ì§ˆë¬¸:
{question}

ì¶œë ¥:
""")
    
    prompt = classification_prompt.format(question=user_input)
    result = GPT_4O_MODEL.invoke(prompt)

    return result.content.strip().lower()


# In[ ]:


# ì§ˆì˜ ëª©ì  : ì‚¬ê³  ê³¼ì‹¤ ë¹„ìœ¨
# SITUATION_CASE['ACCIDENT'] = "accident"
def process_accident(user_input: str) -> str:
    # car_case ë¬¸ì„œ í•„í„°ë§ ë° ì‚¬ê³ ìƒí™© ì¶”ì¶œ
    case_texts = [doc.metadata.get("situation", "") for doc in car_case_docs if doc.metadata.get("situation")]

    # ko-sbert ì„ë² ë”©
    embed_model = SentenceTransformer("jhgan/ko-sbert-nli")
    case_embeddings = embed_model.encode(case_texts)

    # ì‚¬ìš©ì ì…ë ¥
    query_embedding = embed_model.encode([user_input])[0]

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° Top-3 ì¶”ì¶œ
    cos_similarities = np.dot(case_embeddings, query_embedding) / (
        np.linalg.norm(case_embeddings, axis=1) * np.linalg.norm(query_embedding)
    )
    top_k_idx = np.argsort(cos_similarities)[-3:][::-1]
    top_candidates = [car_case_docs[i] for i in top_k_idx]

    # íŒë¡€ ìš”ì•½ ì¶œë ¥
    def summarize(doc, idx):
        return f"{idx+1}. ì‚¬ê±´ ID: {doc.metadata.get('id')}\nì‚¬ê³ ìƒí™©: {doc.metadata.get('situation')}"

    case_summaries = "\n\n".join([summarize(doc, i) for i, doc in enumerate(top_candidates)])

    # GPT - ì‚¬ê±´ID ì„ íƒ(3ê°œ ì¤‘ì— í•˜ë‚˜ íŒë‹¨)
    selection_prompt = PromptTemplate(
        input_variables=["user_input", "case_summaries"],
        template="""
    [ì‚¬ìš©ì ì…ë ¥ ì‚¬ê³  ìƒí™©]
    {user_input}

    [í›„ë³´ íŒë¡€ 3ê±´]
    {case_summaries}

    ìœ„ 3ê±´ ì¤‘, ì‚¬ê³ ì˜ ì „ê°œ êµ¬ì¡°(ì˜ˆ: ì§ì§„ vs ì¢ŒíšŒì „, ë„ë¡œ ì™¸ ì¥ì†Œì—ì„œ ì§„ì…, êµì°¨ë¡œ ë‚´ ì§„ì… ì—¬ë¶€ ë“±)ê°€ ì‚¬ìš©ì ìƒí™©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ **ì‚¬ê±´ ID** í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.

    ë°˜ë“œì‹œ ë‹¤ìŒ ê¸°ì¤€ì„ ê³ ë ¤í•˜ì„¸ìš”:
    - ì°¨ëŸ‰ë“¤ì˜ ìœ„ì¹˜ì™€ ì§„ì… ê²½ë¡œê°€ ìœ ì‚¬í•œê°€?
    - ì‚¬ê³  ë°œìƒ ì§€ì ê³¼ ë°©í–¥ì´ ìœ ì‚¬í•œê°€?
    - ê° ì°¨ëŸ‰ì˜ ì‹ í˜¸Â·ìš°ì„ ê¶Œ ìƒí™©ì´ ìœ ì‚¬í•œê°€?|
    - ë„ë¡œ êµ¬ì¡°(êµì°¨ë¡œ, ì‹ í˜¸ ìœ ë¬´, ë„ë¡œ ì™¸ ì¥ì†Œ ë“±)ê°€ ìœ ì‚¬í•œê°€?

    ì¶œë ¥ í˜•ì‹ (ê³ ì •):
    - ì‚¬ê±´ ID: ì°¨XX-X
    - íŒë‹¨ ê·¼ê±°: (ì„ íƒí•œ ì´ìœ . ë‹¨ìˆœ ìœ ì‚¬ì„±ì´ ì•„ë‹ˆë¼, ì–´ë–¤ ì§€ì ì´ ìœ ì‚¬í–ˆëŠ”ì§€ ëª…í™•íˆ ì„¤ëª…í•  ê²ƒ)
    """
    )
    
    selection_chain = LLMChain(llm=GPT_4O_MODEL, prompt=selection_prompt)
    selection_result = selection_chain.run(user_input=user_input, case_summaries=case_summaries)

    # ì‚¬ê±´ ID íŒŒì‹± ë° ì„ íƒ
    match = re.search(r"ì‚¬ê±´ ID[:ï¼š]?\s*(ì°¨\d{1,2}-\d{1,2})", selection_result)
    selected_id = match.group(1) if match else None
    selected_doc = next((doc for doc in car_case_docs if doc.metadata.get("id") == selected_id), None)

    # ìµœì¢… íŒë‹¨ GPT í”„ë¡¬í”„íŠ¸(ì„ íƒí•œ ì‚¬ê±´ object ë‚´ì—ì„œ ê³¼ì‹¤ë¹„ìœ¨ íŒë‹¨)
    if selected_doc:
        # í•´ë‹¹ ì‚¬ê±´ ê´€ë ¨ ë³´ì¡° ë¬¸ì„œë“¤ì„ í•¨ê»˜ ì „ë‹¬
        related_docs = [doc for doc in car_case_docs if selected_id in doc.page_content]
        context_str = "\n\n".join(doc.page_content for doc in related_docs)

        final_prompt = PromptTemplate(
            input_variables=["user_input", "case_data"],
            template="""
    ë„ˆëŠ” êµí†µì‚¬ê³  ê³¼ì‹¤ íŒë‹¨ ì „ë¬¸ê°€ì•¼.
    ì•„ë˜ 'ì‚¬ê³  ìƒí™©'ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ ìš”ì†Œë¥¼ êµ¬ì¡°í™”í•˜ê³ , ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€(case)ë¥¼ ì°¾ì•„ ê³¼ì‹¤ë¹„ìœ¨ì„ íŒë‹¨í•´ì¤˜.

    ---

    ì‚¬ê³  ìƒí™© ì›ë¬¸:
    {user_input}

    â¤ ì‚¬ê³  ìƒí™© ìš”ì•½ (ë‹¤ìŒ í•­ëª© ê¸°ì¤€):
    - Aì°¨ëŸ‰ ì‹ í˜¸ ë° ì§„í–‰ ë°©ì‹:
    - Bì°¨ëŸ‰ ì‹ í˜¸ ë° ì§„í–‰ ë°©ì‹:
    - ì¶©ëŒ ë°©ì‹ ë° ìœ„ì¹˜:
    - êµì°¨ë¡œ/ì‹ í˜¸ê¸° ìœ ë¬´ ë“± ë„ë¡œ í™˜ê²½:

    ë¬¸ì„œ:
    {case_data}

    ì¶œë ¥ í˜•ì‹ (ê³ ì •):
    1. ê³¼ì‹¤ë¹„ìœ¨: Aì°¨ëŸ‰ xx% vs Bì°¨ëŸ‰ xx%
    2. íŒë‹¨ ê·¼ê±° ìš”ì•½
    3. ì ìš© ë²•ë¥ :
    - [ë²•ë¥ ëª…] ì œ[ì¡°]ì¡° [í•­]
    4. ì°¸ê³  íŒë¡€:
    - [ë²•ì›ëª…] [ì‚¬ê±´ë²ˆí˜¸]

    ì¡°ê±´:
    - ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•´.
    - ìœ ì‚¬ ì‚¬ë¡€ì™€ í˜„ì¬ ì‚¬ê³  ìƒí™©ì´ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´, ì°¨ì´ì ì„ ëª…ì‹œí•˜ê³  ê³¼ì‹¤ë¹„ìœ¨ ì¡°ì • ì´ìœ ë¥¼ ì„¤ëª…í•´.
    - ì¶”ì¸¡ì´ë‚˜ ìƒì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ë¬¸ì„œ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•´.
    """
        )

        chain = LLMChain(llm=GPT_4O_MODEL, prompt=final_prompt)
        response = chain.run(user_input=user_input, case_data=context_str)

        print(f"\nì„ íƒëœ ì‚¬ê±´ ID: {selected_id}")
        print("GPT ìµœì¢… íŒë‹¨ ê²°ê³¼:\n")
        print(response)

    else:
        print("\nâŒ ì‚¬ê±´ IDë¥¼ ì •í™•íˆ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("GPT ì‘ë‹µ:\n", selection_result)


# In[ ]:


# ì§ˆì˜ ëª©ì  : íŒë¡€ ê²€ìƒ‰
# SITUATION_CASE['PRECEDENT'] = "precedent"
def process_precedent(user_input):
    # ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (í•„ìˆ˜!)
    metadata_field_info = [
        AttributeInfo(
            name=METADATA_KEY['PRECEDENT']['COURT'],
            description="íŒë¡€ì˜ ë²•ì›ëª… (ì˜ˆ: ëŒ€ë²•ì›, ì„œìš¸ê³ ë“±ë²•ì› ë“±)",
            type="string"
        ),
        AttributeInfo(
            name=METADATA_KEY['PRECEDENT']['CASE_ID'],
            description="ì‚¬ê±´ë²ˆí˜¸ (ì˜ˆ: 92ë„2077)",
            type="string"
        )
    ]

    # SelfQueryRetriever ìƒì„± (metadata_field_info í•„ìˆ˜)
    self_retriever = SelfQueryRetriever.from_llm(
        llm=GPT_4O_MODEL,
        vectorstore=precedent_db,
        document_contents="êµí†µì‚¬ê³  íŒë¡€ ë°ì´í„°",
        metadata_field_info=metadata_field_info  # âœ… ë°˜ë“œì‹œ í•„ìš”
    )


    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = PromptTemplate(
        input_variables=["question", "context"],
        template="""ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ íŒë¡€ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        
        ë¬¸ì„œ: {context}

        ë‹µë³€ í˜•ì‹:
        - ìš©ì–´/ì¡°í•­ ì •ì˜: [ì •í™•í•œ ì„¤ëª…]
        - ì¶œì²˜ê°€ ëª…ì‹œëœ ê²½ìš°: ê´€ë ¨ ë²•ë¥ /ì¡°ë¬¸ ë²ˆí˜¸/íŒë¡€ëª…ì„ ë°˜ë“œì‹œ í¬í•¨

        ë‹µë³€:
        """
    )

    # QA ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
    qa_chain = RetrievalQA.from_chain_type(
        llm=GPT_4O_MODEL,
        retriever=self_retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": user_input})
    return f"[íŒë¡€ ì„¤ëª… ê²°ê³¼]\n{result['result']}" 


# In[ ]:


# ì§ˆì˜ ëª©ì  : ìš©ì–´ ê²€ìƒ‰
# SITUATION_CASE['TERM'] = "term"
def process_term(user_input):
    # ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (í•„ìˆ˜!)
    metadata_field_info = [
        AttributeInfo(
            name=METADATA_KEY['TERM']['TERM'],
            description="êµí†µì‚¬ê³  ê´€ë ¨ ìš©ì–´ (ì˜ˆ: ë³´í–‰ìì „ìš©ë„ë¡œ, ì°¨ë§ˆ ë“±)",
            type="string"
        )
    ]

    # SelfQueryRetriever ìƒì„± (metadata_field_info í•„ìˆ˜)
    self_retriever = SelfQueryRetriever.from_llm(
        llm=GPT_4O_MODEL,
        vectorstore=term_db,
        document_contents="êµí†µì‚¬ê³  ê´€ë ¨ ìš©ì–´ ë°ì´í„°",
        metadata_field_info=metadata_field_info  # âœ… ë°˜ë“œì‹œ í•„ìš”
    )


    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = PromptTemplate(
        input_variables=["question", "context"],
        template="""ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ìš©ì–´ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        
        ë¬¸ì„œ: {context}

        ë‹µë³€ í˜•ì‹:
        - ìš©ì–´/ì¡°í•­ ì •ì˜: [ì •í™•í•œ ì„¤ëª…]
        - ì¶œì²˜ê°€ ëª…ì‹œëœ ê²½ìš°: ê´€ë ¨ ë²•ë¥ /ì¡°ë¬¸ ë²ˆí˜¸/íŒë¡€ëª…ì„ ë°˜ë“œì‹œ í¬í•¨

        ë‹µë³€:
        """
    )

    # QA ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
    qa_chain = RetrievalQA.from_chain_type(
        llm=GPT_4O_MODEL,
        retriever=self_retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": user_input})
    return f"[ìš©ì–´ ì„¤ëª… ê²°ê³¼]\n{result['result']}" 


# In[ ]:


# ì§ˆì˜ ëª©ì  : ë„ë¡œêµí†µë²•ë²• ê²€ìƒ‰
# SITUATION_CASE['TERM'] = "term"
def process_load_traffic_law(user_input):
    # ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (í•„ìˆ˜!)
    metadata_field_info = [
    AttributeInfo(
        name="ë²•ë¥ ì¡°ë¬¸",
        description="ë²•ë¥ ì˜ ì¡°ë¬¸ ë²ˆí˜¸ (ì˜ˆ: ì œ5ì¡°, ì œ8ì¡°)",
        type="string"
    ),
    AttributeInfo(
        name="ì¡°í•­ëª…",
        description="ì¡°í•­ì˜ ì œëª© (ì˜ˆ: ì‹ í˜¸ ë˜ëŠ” ì§€ì‹œì— ë”°ë¥¼ ì˜ë¬´, ë³´í–‰ìì˜ í†µí–‰)",
        type="string"
    ),
    AttributeInfo(
        name="í•­ë²ˆí˜¸",
        description="ì¡°í•­ ë‚´ í•­ ë²ˆí˜¸ (ì˜ˆ: 1, 2, 3, ...)",
        type="integer"
    ),
    AttributeInfo(
        name="ì „ì²´ì°¸ì¡°",
        description="ì¡°ë¬¸ê³¼ í•­ì„ í•©ì¹œ ì „ì²´ ì°¸ì¡° (ì˜ˆ: ì œ5ì¡° 1í•­)",
        type="string"
    ),
]

    # SelfQueryRetriever ìƒì„± (metadata_field_info í•„ìˆ˜)
    self_retriever = SelfQueryRetriever.from_llm(
        llm=GPT_4O_MODEL,
        vectorstore=load_traffic_law_db,
        document_contents="ë„ë¡œêµí†µë²• ì¡°ë¬¸ ë° í•­ì˜ ì£¼ìš” ë‚´ìš©",
        metadata_field_info=metadata_field_info  # âœ… ë°˜ë“œì‹œ í•„ìš”
    )


    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = PromptTemplate(
        input_variables=["question", "context"],
        template="""ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë„ë¡œêµí†µë²• ë‚´ìš©ìš©ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        
        ë¬¸ì„œ: {context}

        ë‹µë³€ í˜•ì‹:
        - ìš©ì–´/ì¡°í•­ ì •ì˜: [ì •í™•í•œ ì„¤ëª…]
        - ì¶œì²˜ê°€ ëª…ì‹œëœ ê²½ìš°: ê´€ë ¨ ë²•ë¥ /ì¡°ë¬¸ ë²ˆí˜¸/íŒë¡€ëª…ì„ ë°˜ë“œì‹œ í¬í•¨

        ë‹µë³€:
        """
    )

    # QA ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
    qa_chain = RetrievalQA.from_chain_type(
        llm=GPT_4O_MODEL,
        retriever=self_retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

    result = qa_chain.invoke({"query": user_input})
    return f"[ë„ë¡œêµí†µë²•ë¡œêµí†µë²• ì„¤ëª… ê²°ê³¼]\n{result['result']}" 


# In[ ]:


def process_general(user_input):
    general_prompt = PromptTemplate.from_template("""
ë„ˆëŠ” êµí†µì‚¬ê³  ìƒë‹´ ì „ë¬¸ AI ì±—ë´‡ì´ì•¼.

êµí†µì‚¬ê³  íŒë¡€, ë„ë¡œêµí†µë²•, ë²•ë¥  ìš©ì–´ ë“±ì— ëŒ€í•´ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ëŠ” ì—­í• ì„ í•´.

í•˜ì§€ë§Œ ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì—ì„œë„ í˜¼ì ìœ ì—°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•´:
- ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ìš©ì–´, ë²•ë¥  ì¡°í•­, íŒë¡€ ë²ˆí˜¸ê°€ ë‚˜ì™”ì„ ê²½ìš°
- ë¬¸ì„œì— ì—†ëŠ” ì§ˆë¬¸ì´ë¼ë„, ë„ˆì˜ ì§€ì‹ê³¼ ìƒì‹ìœ¼ë¡œ ì„¤ëª…ì´ ê°€ëŠ¥í•œ ê²½ìš°
- ë¬¸ì„œì™€ ìƒê´€ì—†ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ (ì˜ˆ: ìê¸°ì†Œê°œ, ì¸ê³µì§€ëŠ¥, ë‚ ì”¨ ë“±)

ì´ëŸ´ ë• â€œë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤â€ ê°™ì€ ë§ì€ í•˜ì§€ ë§ê³ ,
AI ì±—ë´‡ìœ¼ë¡œì„œ ë„ˆì˜ ì§€ì‹ìœ¼ë¡œ ìµœëŒ€í•œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì¤˜.

ì§ˆë¬¸:
{question}

ë‹µë³€:
""")    
    
    prompt = general_prompt.format(question=user_input)    
    result = GPT_3_5_MODEL.invoke(prompt)
    
    return result.content.strip()


# # Main

# In[ ]:


SITUATION_CASE = {
    'GENERAL' : "general",

    'ACCIDENT' : "accident",
    'TERM' : "term",
    'PRECEDENT' : "precedent",
    'LAW' : "law",
}
# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš— êµí†µì‚¬ê³  AI ë¶„ì„ê¸°ì…ë‹ˆë‹¤.")
    print("ì‚¬ê³  ìƒí™©ì´ë‚˜ ì•Œê³  ì‹¶ì€ ë²•ë¥ /íŒë¡€ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    user_input = input("ì…ë ¥ > ").strip()

    if user_input:
        category = classify_query(user_input)
        print(category)
        
        if (category == SITUATION_CASE['ACCIDENT']):
            response = process_accident(user_input)
        elif (category == SITUATION_CASE['TERM']):
            response = process_term(user_input)
        elif (category == SITUATION_CASE['PRECEDENT']):
            response = process_precedent(user_input)
        elif (category == SITUATION_CASE['LAW']):
            response = process_load_traffic_law(user_input)
        else:
            response = process_general(user_input)
        print("\nğŸ“˜ ê²°ê³¼ ì¶œë ¥:\n")
        print(response)
    else:
        print("âŒ ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")


# In[ ]:




