"""
êµí†µì‚¬ê³  ì±—ë´‡ ì§ˆë¬¸ ë¶„ë¥˜ ì„œë¹„ìŠ¤ ë° RAG ì²˜ë¦¬
íŒŒì¸íŠœë‹ëœ GPT-3.5-turbo ëª¨ë¸ ì‚¬ìš© + OpenAI ì„ë² ë”© í†µì¼
"""

import openai
import os
import logging
from typing import Dict, Any, List
from django.conf import settings
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers import SelfQueryRetriever
from ..utils.vector_db import get_vector_db_manager

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class TrafficAccidentClassifier:
    """
    íŒŒì¸íŠœë‹ëœ GPT-3.5-turboë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ ë¶„ë¥˜ê¸° + RAG ì²˜ë¦¬ê¸°
    
    ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬:
    - accident: êµí†µì‚¬ê³  ìƒí™© ë¶„ì„
    - precedent: íŒë¡€ ê²€ìƒ‰
    - law: ë„ë¡œêµí†µë²• ì¡°íšŒ
    - term: ìš©ì–´ ì„¤ëª…
    - general: ì¼ë°˜ ì§ˆë¬¸
    """
    
    # í—ˆìš©ë˜ëŠ” ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬
    VALID_CATEGORIES = {'accident', 'precedent', 'law', 'term', 'general'}
    
    # í´ë°± ë¶„ë¥˜ (í‚¤ì›Œë“œ ê¸°ë°˜)
    FALLBACK_KEYWORDS = {
        'accident': [
            'ì‚¬ê³ ', 'ì¶©ëŒ', 'ì ‘ì´‰', 'ê³¼ì‹¤', 'ë¹„ìœ¨', 'ì¢ŒíšŒì „', 'ì§ì§„', 'êµì°¨ë¡œ',
            'ì‹ í˜¸ìœ„ë°˜', 'ì£¼ì°¨ì¥', 'í›„ì§„', 'ì°¨ë¡œë³€ê²½', 'ì¶”ëŒ', 'ì¸¡ë©´ì¶©ëŒ'
        ],
        'precedent': [
            'íŒë¡€', 'ëŒ€ë²•ì›', 'ê³ ë“±ë²•ì›', 'ì§€ë°©ë²•ì›', 'íŒê²°', 'ì‚¬ê±´ë²ˆí˜¸',
            'íŒë‹¨', 'ìš”ì§€', 'ë²•ì›', 'ì†Œì†¡', 'ì¬íŒ'
        ],
        'law': [
            'ë„ë¡œêµí†µë²•', 'ë²•ë¥ ', 'ì¡°ë¬¸', 'ì œ', 'ì¡°', 'í•­', 'ê·œì •', 'ìœ„ë°˜',
            'ì²˜ë²Œ', 'ë²”ì¹™ê¸ˆ', 'ë²Œì ', 'ë²•ì ', 'ê·œì¹™'
        ],
        'term': [
            'ì •ì˜', 'ì˜ë¯¸', 'ëœ»', 'ìš©ì–´', 'ì„¤ëª…', 'ê°œë…', 'ë¬´ì—‡', 'ì–´ë–¤',
            'ì°¨ë¡œ', 'ë„ë¡œ', 'ì°¨ëŸ‰', 'ìš´ì „ì', 'ë³´í–‰ì'
        ]
    }
    
    # JSON íŒŒì¼ KEY ê°’ ì •ì˜ (UI.pyì™€ ë™ì¼)
    METADATA_KEY = {
        'PRECEDENT': {
            'COURT': "court",
            'CASE_ID': "case_id",
            'CONTENT': "content",
        },
        'LOAD_TRAFFIC_LAW': {
            'ARTICLE_TITLE': "article_title",
            'ARTICLE_NUMBER': "article_number", 
            'PARAGRAPH': "paragraph",
            'CONTENT': "content",
        }
    }
    
    def __init__(self):
        """ë¶„ë¥˜ê¸° ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = openai.OpenAI(
                api_key=os.getenv('OPENAI_API_KEY')
            )
            
            # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ID
            self.model_id = os.getenv('FINETUNED_MODEL_ID')
            
            if not self.model_id:
                logger.warning("FINETUNED_MODEL_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í´ë°± ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            
            # GPT ëª¨ë¸ ì´ˆê¸°í™” (RAGìš©)
            self.gpt_4o_model = ChatOpenAI(
                model="gpt-4o-mini", 
                temperature=0,
                api_key=os.getenv('OPENAI_API_KEY')
            )
            
            # VectorDB ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.vector_db_manager = get_vector_db_manager()
            
            logger.info(f"TrafficAccidentClassifier ì´ˆê¸°í™” ì™„ë£Œ - ëª¨ë¸: {self.model_id}")
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def classify_query(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        
        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            str: ë¶„ë¥˜ ê²°ê³¼ ('accident', 'precedent', 'law', 'term', 'general')
        """
        import time
        start_time = time.time()
        
        if not user_input or not user_input.strip():
            return 'general'
        
        # 1ì°¨: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ë¶„ë¥˜ ì‹œë„
        try:
            logger.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¶„ë¥˜ ì‹œì‘: '{user_input[:30]}...'")
            api_start = time.time()
            
            category = self._classify_with_finetuned_model(user_input.strip())
            
            api_time = time.time() - api_start
            logger.info(f"API í˜¸ì¶œ ì‹œê°„: {api_time:.2f}ì´ˆ")
            
            if self._is_valid_category(category):
                total_time = time.time() - start_time
                logger.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¶„ë¥˜ ì„±ê³µ: '{user_input[:30]}...' â†’ {category} (ì´ {total_time:.2f}ì´ˆ)")
                return category
        except Exception as e:
            logger.warning(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        
        # 2ì°¨: í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¶„ë¥˜
        fallback_start = time.time()
        category = self._fallback_classify(user_input.strip())
        fallback_time = time.time() - fallback_start
        
        total_time = time.time() - start_time
        logger.info(f"í´ë°± ë¶„ë¥˜ ì‚¬ìš©: '{user_input[:30]}...' â†’ {category} (í´ë°±: {fallback_time:.2f}ì´ˆ, ì´: {total_time:.2f}ì´ˆ)")
        return category
    
    def process_precedent(self, user_input: str) -> str:
        """
        íŒë¡€ ê²€ìƒ‰ ë° ë¶„ì„ (OpenAI ì„ë² ë”© í†µì¼ ë°©ì‹)
        
        Args:
            user_input (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: íŒë¡€ ê²€ìƒ‰ ê²°ê³¼
        """
        import time
        start_time = time.time()
        
        try:
            logger.info(f"íŒë¡€ ê²€ìƒ‰ ì‹œì‘: '{user_input[:30]}...'")
            
            # 1. VectorDBì—ì„œ precedent ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            precedent_db = self.vector_db_manager.get_vector_db('precedent')
            if not precedent_db:
                logger.error("precedent VectorDBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._generate_precedent_fallback_response(user_input)
            
            # 2. ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (Self-Query Retrieverìš©)
            metadata_field_info = [
                AttributeInfo(
                    name=self.METADATA_KEY['PRECEDENT']['COURT'],
                    description="íŒë¡€ì˜ ë²•ì›ëª… (ì˜ˆ: ëŒ€ë²•ì›, ì„œìš¸ê³ ë“±ë²•ì›, ì§€ë°©ë²•ì› ë“±)",
                    type="string"
                ),
                AttributeInfo(
                    name=self.METADATA_KEY['PRECEDENT']['CASE_ID'],
                    description="ì‚¬ê±´ë²ˆí˜¸ (ì˜ˆ: 92ë„2077, 2019ë‹¤12345, 2015ë‚˜60480 ë“±)",
                    type="string"
                )
            ]
            
            # 3. Self-Query Retriever ìƒì„±
            self_retriever = SelfQueryRetriever.from_llm(
                llm=self.gpt_4o_model,
                vectorstore=precedent_db,
                document_contents="êµí†µì‚¬ê³  ê´€ë ¨ ë²•ì› íŒë¡€ ë°ì´í„°",
                metadata_field_info=metadata_field_info
            )
            
            # 4. íŒë¡€ ê²€ìƒ‰ ë° ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ê°œì„ ëœ ì‚¬ìš©ì ì¹œí™”ì  í¬ë§·)
            prompt = PromptTemplate(
                input_variables=["question", "context"],
                template="""
ë„ˆëŠ” êµí†µì‚¬ê³  íŒë¡€ë¥¼ ìš”ì•½ ì •ë¦¬í•´ì£¼ëŠ” ì „ë¬¸ê°€ì•¼.

ì•„ë˜ ë¬¸ì„œ(context)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ëœ íŒë¡€ë¥¼ **ê°„ê²°í•˜ê³  ì½ê¸° ì‰½ê²Œ** ì„¤ëª…í•´ì¤˜.

---
ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ íŒë¡€ ë¬¸ì„œ:
{context}
---

ì¶œë ¥ í˜•ì‹:

âš–ï¸ **íŒë¡€ ê²€ìƒ‰ ê²°ê³¼**

**ğŸ” ê²€ìƒ‰ ë‚´ìš©**: [ì‚¬ìš©ì ì§ˆë¬¸ ìš”ì•½]

**ğŸ“‹ ê´€ë ¨ íŒë¡€**:

**1ï¸âƒ£ [ì‚¬ê±´ë²ˆí˜¸]**
â€¢ **ë²•ì›**: [ë²•ì›ëª…]
â€¢ **ì‚¬ê³  ê°œìš”**: [ì‚¬ê³  ìƒí™©ì„ 1-2ì¤„ë¡œ ê°„ë‹¨íˆ]
â€¢ **ì£¼ìš” íŒë‹¨**: [í•µì‹¬ ë²•ì  íŒë‹¨ 1-2ì¤„]
â€¢ **ê³¼ì‹¤ë¹„ìœ¨**: [Aì°¨ëŸ‰ XX% vs Bì°¨ëŸ‰ XX%] *(ëª…ì‹œëœ ê²½ìš°ë§Œ)*

**2ï¸âƒ£ [ì‚¬ê±´ë²ˆí˜¸]**
â€¢ **ë²•ì›**: [ë²•ì›ëª…]
â€¢ **ì‚¬ê³  ê°œìš”**: [ì‚¬ê³  ìƒí™©ì„ 1-2ì¤„ë¡œ ê°„ë‹¨íˆ]
â€¢ **ì£¼ìš” íŒë‹¨**: [í•µì‹¬ ë²•ì  íŒë‹¨ 1-2ì¤„]
â€¢ **ê³¼ì‹¤ë¹„ìœ¨**: [ëª…ì‹œëœ ê²½ìš°ë§Œ í‘œì‹œ]

*(ìµœëŒ€ 3-4ê°œ íŒë¡€ë§Œ í‘œì‹œ)*

**ğŸ’¡ ì°¸ê³ ì‚¬í•­**:
- êµ¬ì²´ì ì¸ ì‚¬ê³  ìƒí™©ì— ë”°ë¼ ê³¼ì‹¤ë¹„ìœ¨ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìƒì„¸í•œ ì‚¬ê³  ê²½ìœ„ê°€ í•„ìš”í•©ë‹ˆë‹¤

**ì¡°ê±´**:
- ê° íŒë¡€ë¥¼ **ëª…í™•íˆ êµ¬ë¶„**í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”
- **ê³¼ì‹¤ë¹„ìœ¨**ì€ ëª…ì‹œëœ ê²½ìš°ë§Œ í‘œì‹œí•˜ê³ , ì—†ìœ¼ë©´ ìƒëµí•˜ì„¸ìš”
- **ê¸´ ë‚´ìš©ì€ 1-2ì¤„ë¡œ ìš”ì•½**í•˜ì—¬ ì½ê¸° ì‰½ê²Œ í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- ì‚¬ê±´ë²ˆí˜¸ì™€ ë²•ì›ëª…ì„ **ì •í™•íˆ** í‘œì‹œí•˜ì„¸ìš”

ë‹µë³€:
"""            )
            
            # 5. QA ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.gpt_4o_model,
                retriever=self_retriever,   # ìœ ì‚¬ë„ ê²€ìƒ‰
                chain_type="stuff",
                chain_type_kwargs={"prompt": prompt}
            )
            
            # 6. ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± (ë¡œê¹… ì¶”ê°€)
            retrieval_start = time.time()
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            if logger.isEnabledFor(logging.INFO):
                # ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ìƒìœ„ ê²°ê³¼ í™•ì¸
                docs_with_scores = precedent_db.similarity_search_with_score(user_input, k=4)
                logger.info(f"ë²•ë¥  ê²€ìƒ‰ ìœ ì‚¬ë„: {[(doc.metadata.get('article_number', 'N/A'), f'{score:.3f}') for doc, score in docs_with_scores]}")
            
            result = qa_chain.invoke({"query": user_input})
            retrieval_time = time.time() - retrieval_start
            
            total_time = time.time() - start_time
            logger.info(f"íŒë¡€ ê²€ìƒ‰ ì™„ë£Œ: '{user_input[:30]}...' (ê²€ìƒ‰: {retrieval_time:.2f}ì´ˆ, ì´: {total_time:.2f}ì´ˆ)")
            
            return result['result']
            
        except Exception as e:
            logger.error(f"íŒë¡€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._generate_precedent_fallback_response(user_input)
    
    def process_law(self, user_input: str) -> str:
        """
        ë„ë¡œêµí†µë²• ì¡°íšŒ ë° ë¶„ì„ (OpenAI ì„ë² ë”© í†µì¼ ë°©ì‹)
        
        Args:
            user_input (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            str: ë„ë¡œêµí†µë²• ì¡°íšŒ ê²°ê³¼
        """
        import time
        start_time = time.time()
        
        try:
            logger.info(f"ë„ë¡œêµí†µë²• ì¡°íšŒ ì‹œì‘: '{user_input[:30]}...'")
            
            # 1. VectorDBì—ì„œ traffic_law_rag ì»´ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            law_db = self.vector_db_manager.get_vector_db('traffic_law_rag')
            if not law_db:
                logger.error("traffic_law_rag VectorDBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._generate_law_fallback_response(user_input)
            
            # 2. ë©”íƒ€ë°ì´í„° í•„ë“œ ì •ì˜ (Self-Query Retrieverìš©)
            metadata_field_info = [
                AttributeInfo(
                    name="article_title",
                    description="ë„ë¡œêµí†µë²• ì¡°ë¬¸ ì „ì²´ ì œëª© (ì˜ˆ: ì œ5ì¡°(ì‹ í˜¸ ë˜ëŠ” ì§€ì‹œì— ë”°ë¥¼ ì˜ë¬´), ì œ25ì¡°(êµì°¨ë¡œ í†µí–‰ë°©ë²•))",
                    type="string"
                ),
                AttributeInfo(
                    name="article_number",
                    description="ë„ë¡œêµí†µë²• ì¡°ë¬¸ ë²ˆí˜¸ (ì˜ˆ: ì œ5ì¡°, ì œ25ì¡°, ì œ27ì¡° ë“±)",
                    type="string"
                ),
                AttributeInfo(
                    name="article_name",
                    description="ë„ë¡œêµí†µë²• ì¡°ë¬¸ëª… (ì˜ˆ: ì‹ í˜¸ ë˜ëŠ” ì§€ì‹œì— ë”°ë¥¼ ì˜ë¬´, êµì°¨ë¡œ í†µí–‰ë°©ë²• ë“±)",
                    type="string"
                ),
                AttributeInfo(
                    name="subsection_title",
                    description="ì¡°ë¬¸ ë‚´ í•­ ì œëª© (ì˜ˆ: ì œ5ì¡° 1í•­, ì œ25ì¡° 2í•­ ë“±)",
                    type="string"
                ),
                AttributeInfo(
                    name="keywords",
                    description="ì¡°ë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ (ì˜ˆ: ì‹ í˜¸, ì§€ì‹œ, êµí†µì•ˆì „ì‹œì„¤, ê²½ì°°ê³µë¬´ì› ë“±)",
                    type="string"
                )
            ]
            
            # 3. Self-Query Retriever ìƒì„±
            self_retriever = SelfQueryRetriever.from_llm(
                llm=self.gpt_4o_model,
                vectorstore=law_db,
                document_contents="ë„ë¡œêµí†µë²• ì¡°ë¬¸ ë‚´ìš© ë° í•´ì„ ë°ì´í„°",
                metadata_field_info=metadata_field_info,
                search_kwargs={"k": 5}  # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¦ê°€
            )
            
            # 4. ë„ë¡œêµí†µë²• ì¡°íšŒ ë° ì„¤ëª… í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì ì¹œí™”ì  í¬ë§·)
            prompt = PromptTemplate(
                input_variables=["question", "context"],
                template="""
ë„ˆëŠ” ë„ë¡œêµí†µë²•ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì•¼.

ì•„ë˜ ë¬¸ì„œ(context)ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ëœ ë„ë¡œêµí†µë²• ì¡°ë¬¸ì„ **ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ë¡­ê²Œ** ì„¤ëª…í•´ì¤˜.

---
ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë„ë¡œêµí†µë²• ë¬¸ì„œ:
{context}
---

ì¶œë ¥ í˜•ì‹:

ğŸ“š **ë„ë¡œêµí†µë²• ì¡°íšŒ ê²°ê³¼**

**ğŸ” ì¡°íšŒ ë‚´ìš©**: [ì‚¬ìš©ì ì§ˆë¬¸ ìš”ì•½]

**ğŸ“‹ ê´€ë ¨ ì¡°ë¬¸**:

**1ï¸âƒ£ [ì¡°ë¬¸ ì œëª©] - [ì¡°ë¬¸ë²ˆí˜¸]**
â€¢ **ì£¼ìš” ë‚´ìš©**: [ì£¼ìš” ë‚´ìš©ì„ 1-2ì¤„ë¡œ ê°„ë‹¨íˆ]
â€¢ **ìƒì„¸ ì„¤ëª…**: 
  - [ì£¼ìš” ìš”ì  1]
  - [ì£¼ìš” ìš”ì  2]
  - [ì£¼ìš” ìš”ì  3 (ìˆëŠ” ê²½ìš°ë§Œ)]
â€¢ **ìœ„ë°˜ ì‹œ ì²˜ë²Œ**: [ë²”ì¹™ê¸ˆ, ë²Œì  ë“± ëª…ì‹œëœ ê²½ìš°ë§Œ]

**2ï¸âƒ£ [ì¡°ë¬¸ ì œëª©] - [ì¡°ë¬¸ë²ˆí˜¸]**
â€¢ **ì£¼ìš” ë‚´ìš©**: [ì£¼ìš” ë‚´ìš©ì„ 1-2ì¤„ë¡œ ê°„ë‹¨íˆ]
â€¢ **ìƒì„¸ ì„¤ëª…**: [ì£¼ìš” ìš”ì ë“¤ì„ ì§ˆëŒ€ë³„ë¡œ ì •ë¦¬]
â€¢ **ìœ„ë°˜ ì‹œ ì²˜ë²Œ**: [ëª…ì‹œëœ ê²½ìš°ë§Œ í‘œì‹œ]

*(ìµœëŒ€ 3-4ê°œ ì¡°ë¬¸ë§Œ í‘œì‹œ)*

**ğŸ’¡ ì°¸ê³ ì‚¬í•­**:
- êµ¬ì²´ì ì¸ ì‚¬ê³  ìƒí™©ì— ë”°ë¼ ì ìš©ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì „ë¬¸ê°€ ìƒë‹´ì„ ë°›ì•„ë³´ì„¸ìš”

**ì¡°ê±´**:
- ê° ì¡°ë¬¸ì„ **ëª…í™•íˆ êµ¬ë¶„**í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”
- **ëŒ€ì¤‘ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´**ë¡œ ì„¤ëª…í•˜ì„¸ìš”
- **ê¸´ ë‚´ìš©ì€ 1-2ì¤„ë¡œ ìš”ì•½**í•˜ì—¬ ì½ê¸° ì‰¬ë¡­ê²Œ í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì„ì˜ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- ì¡°ë¬¸ë²ˆí˜¸ì™€ ì œëª©ì„ **ì •í™•íˆ** í‘œì‹œí•˜ì„¸ìš”
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
- ì‚¬ìš©ìê°€ íŠ¹ì • ì¡°ë¬¸ì„ ë¬¼ì–´ë´¤ë‹¤ë©´, í•´ë‹¹ ì¡°ë¬¸ì„ ìš°ì„ ì ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”

ë‹µë³€:
"""
            )
            
            # 5. QA ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.gpt_4o_model,
                retriever=self_retriever,   # ìœ ì‚¬ë„ ê²€ìƒ‰
                chain_type="stuff",
                chain_type_kwargs={"prompt": prompt}
            )
            
            # 6. ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±
            retrieval_start = time.time()
            result = qa_chain.invoke({"query": user_input})
            retrieval_time = time.time() - retrieval_start
            
            total_time = time.time() - start_time
            logger.info(f"ë„ë¡œêµí†µë²• ì¡°íšŒ ì™„ë£Œ: '{user_input[:30]}...' (ê²€ìƒ‰: {retrieval_time:.2f}ì´ˆ, ì´: {total_time:.2f}ì´ˆ)")
            
            return result['result']
            
        except Exception as e:
            logger.error(f"ë„ë¡œêµí†µë²• ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._generate_law_fallback_response(user_input)
    
    def process_user_query(self, user_input: str) -> tuple:
        """
        í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ (ë¶„ë¥˜ + ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬)
        
        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            tuple: (category, response)
        """
        try:
            # 1. ì§ˆë¬¸ ë¶„ë¥˜
            category = self.classify_query(user_input)
            
            # 2. ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
            if category == 'precedent':
                response = self.process_precedent(user_input)
            elif category == 'law':
                response = self.process_law(user_input)
            elif category == 'accident':
                response = self._process_accident_placeholder(user_input)
            elif category == 'term':
                response = self._process_term_placeholder(user_input)
            else:  # general
                response = self._process_general_placeholder(user_input)
            
            return category, response
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 'general', self._generate_error_response(user_input, str(e))
    
    def _classify_with_finetuned_model(self, user_input: str) -> str:
        """
        íŒŒì¸íŠœë‹ëœ GPT-3.5-turbo ëª¨ë¸ë¡œ ë¶„ë¥˜
        
        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            str: ë¶„ë¥˜ ê²°ê³¼
            
        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        if not self.model_id:
            raise Exception("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ IDê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        try:
            # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.model_id,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¤ìŒ ì§ˆë¬¸ì„ accident, precedent, law, term, general ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": user_input
                    }
                ],
                max_tokens=10,
                temperature=0.0,
                timeout=5
            )
            
            # ì‘ë‹µ íŒŒì‹±
            category = response.choices[0].message.content.strip().lower()
            
            # ì‘ë‹µ ê²€ì¦
            if not self._is_valid_category(category):
                logger.warning(f"íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ ë°˜í™˜: {category}")
                raise Exception(f"ì˜ëª»ëœ ë¶„ë¥˜ ê²°ê³¼: {category}")
            
            return category
            
        except openai.APITimeoutError:
            logger.error("OpenAI API íƒ€ì„ì•„ì›ƒ")
            raise Exception("API íƒ€ì„ì•„ì›ƒ")
        except openai.APIError as e:
            logger.error(f"OpenAI API ì—ëŸ¬: {str(e)}")
            raise Exception(f"API ì—ëŸ¬: {str(e)}")
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}")
            raise
    
    def _fallback_classify(self, user_input: str) -> str:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¶„ë¥˜
        
        Args:
            user_input (str): ì‚¬ìš©ì ì…ë ¥
            
        Returns:
            str: ë¶„ë¥˜ ê²°ê³¼
        """
        user_input_lower = user_input.lower()
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        category_scores = {}
        
        for category, keywords in self.FALLBACK_KEYWORDS.items():
            score = 0
            for keyword in keywords:
                if keyword in user_input_lower:
                    score += 1
            category_scores[category] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if category_scores and max(category_scores.values()) > 0:
            best_category = max(category_scores.items(), key=lambda x: x[1])[0]
            logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼: {category_scores}, ì„ íƒ: {best_category}")
            return best_category
        
        # ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ general
        return 'general'
    
    def _is_valid_category(self, category: str) -> bool:
        """
        ìœ íš¨í•œ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ì¸ì§€ í™•ì¸
        
        Args:
            category (str): ë¶„ë¥˜ ê²°ê³¼
            
        Returns:
            bool: ìœ íš¨ ì—¬ë¶€
        """
        return category and category.lower() in self.VALID_CATEGORIES
    
    def _generate_precedent_fallback_response(self, user_input: str) -> str:
        """íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ (ê°œì„ ëœ í¬ë§·)"""
        return f"""âš–ï¸ **íŒë¡€ ê²€ìƒ‰ ê²°ê³¼**

**ğŸ” ê²€ìƒ‰ ë‚´ìš©**: "{user_input}"

**âš ï¸ ì¼ì‹œì  ì˜¤ë¥˜ ë°œìƒ**

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ íŒë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”**:

**ğŸ¯ êµ¬ì²´ì ì¸ ì‚¬ê±´ë²ˆí˜¸ë¡œ ê²€ìƒ‰**
â€¢ "ëŒ€ë²•ì› 2019ë‹¤12345 íŒë¡€ ë‚´ìš©ì€?"
â€¢ "ì„œìš¸ê³ ë“±ë²•ì› 2015ë‚˜60480 íŒë¡€ ê²€ìƒ‰"

**ğŸ¯ ì‚¬ê³  ìœ í˜•ê³¼ í•¨ê»˜ ê²€ìƒ‰**
â€¢ "êµì°¨ë¡œ ì¢ŒíšŒì „ ì‚¬ê³  íŒë¡€"
â€¢ "ì‹ í˜¸ìœ„ë°˜ ê´€ë ¨ íŒë¡€"
â€¢ "ì£¼ì°¨ì¥ ì ‘ì´‰ì‚¬ê³  íŒë¡€"

**ğŸ¯ ë²•ì›ë³„ ê²€ìƒ‰**
â€¢ "ëŒ€ë²•ì› êµí†µì‚¬ê³  íŒë¡€"
â€¢ "ê³ ë“±ë²•ì› ê³¼ì‹¤ë¹„ìœ¨ íŒë¡€"

**ğŸ“ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!**"""
    
    def _generate_law_fallback_response(self, user_input: str) -> str:
        """ë„ë¡œêµí†µë²• ì¡°íšŒ ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ (ê°œì„ ëœ í¬ë§·)"""
        return f"""ğŸ“š **ë„ë¡œêµí†µë²• ì¡°íšŒ ê²°ê³¼**

**ğŸ” ì¡°íšŒ ë‚´ìš©**: "{user_input}"

**âš ï¸ ì¼ì‹œì  ì˜¤ë¥˜ ë°œìƒ**

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë„ë¡œêµí†µë²• ì¡°íšŒ ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”**:

**ğŸ¯ êµ¬ì²´ì ì¸ ì¡°ë¬¸ë²ˆí˜¸ë¡œ ê²€ìƒ‰**
â€¢ "ë„ë¡œêµí†µë²• ì œ5ì¡° ë‚´ìš©ì€?"
â€¢ "ì œ25ì¡° êµì°¨ë¡œ í†µí–‰ë°©ë²• ì•Œë ¤ì£¼ì„¸ìš”"

**ğŸ¯ í‚¤ì›Œë“œì™€ í•¨ê»˜ ê²€ìƒ‰**
â€¢ "ì‹ í˜¸ìœ„ë°˜ ì²˜ë²Œ ê·œì •"
â€¢ "êµì°¨ë¡œ í†µí–‰ ê·œì¹™"
â€¢ "ì°¨ë¡œë³€ê²½ ê´€ë ¨ ë²•ë¥ "

**ğŸ¯ ìƒí™©ë³„ ê²€ìƒ‰**
â€¢ "ì¢ŒíšŒì „ ê´€ë ¨ ë„ë¡œêµí†µë²•"
â€¢ "ì†ë„ìœ„ë°˜ ë²Œê¸ˆ ê·œì •"
â€¢ "ë³´í–‰ì ë³´í˜¸ ì˜ë¬´ ë²•ë¥ "

**ğŸ“ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!**"""
    
    def _process_accident_placeholder(self, user_input: str) -> str:
        """ì‚¬ê³  ë¶„ì„ í”Œë ˆì´ìŠ¤í™€ë” (í–¥í›„ êµ¬í˜„) - ê°œì„ ëœ í¬ë§·"""
        return f"""ğŸš— **êµí†µì‚¬ê³  ë¶„ì„ ê²°ê³¼**

**ğŸ” ë¶„ì„ ëŒ€ìƒ**: "{user_input}"

**ğŸ”§ ì¤€ë¹„ ì¤‘ì¸ ê¸°ëŠ¥**

ì•ˆë…•í•˜ì„¸ìš”! êµí†µì‚¬ê³  ê³¼ì‹¤ë¹„ìœ¨ ë¶„ì„ ê¸°ëŠ¥ì„ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.

**ğŸ’¡ ê³§ ì œê³µë  ì„œë¹„ìŠ¤**:
â€¢ **ê³¼ì‹¤ë¹„ìœ¨ ìë™ ê³„ì‚°** (Aì°¨ëŸ‰ XX% vs Bì°¨ëŸ‰ XX%)
â€¢ **ë²•ì  ê·¼ê±° ì œì‹œ** (ë„ë¡œêµí†µë²• ì¡°ë¬¸ + íŒë¡€)
â€¢ **ì¡°ì • ìš”ì†Œ ë¶„ì„** (ì‹ í˜¸ìœ„ë°˜, ì†ë„ìœ„ë°˜ ë“±)
â€¢ **ìƒì„¸í•œ ì‚¬ê³  ë¶„ì„** ë° ì£¼ì˜ì‚¬í•­

**ğŸ“‹ í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥**:
â€¢ âš–ï¸ **íŒë¡€ ê²€ìƒ‰** âœ… (ì™„ë£Œ)
â€¢ ğŸ“š **ë„ë¡œêµí†µë²• ì¡°íšŒ** âœ… (ì™„ë£Œ)

**ğŸ¯ ì˜ˆì‹œ ì§ˆë¬¸**:
â€¢ "ëŒ€ë²•ì› 2019ë‹¤12345 íŒë¡€ ë‚´ìš©ì€?"
â€¢ "êµì°¨ë¡œ ì¢ŒíšŒì „ ì‚¬ê³  íŒë¡€ ê²€ìƒ‰"

**ë¹ ë¥¸ ì‹œì¼ ë‚´ì— êµí†µì‚¬ê³  ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!** ğŸš€"""
    
    def _process_term_placeholder(self, user_input: str) -> str:
        """ìš©ì–´ ì„¤ëª… í”Œë ˆì´ìŠ¤í™€ë” (í–¥í›„ êµ¬í˜„) - ê°œì„ ëœ í¬ë§·"""
        return f"""ğŸ“– **ìš©ì–´ ì„¤ëª… ê²°ê³¼**

**ğŸ” ì§ˆë¬¸ ë‚´ìš©**: "{user_input}"

**ğŸ”§ ì¤€ë¹„ ì¤‘ì¸ ê¸°ëŠ¥**

êµí†µì‚¬ê³  ìš©ì–´ ì„¤ëª… ê¸°ëŠ¥ì„ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.

**ğŸ’¡ ê³§ ì œê³µë  ì„œë¹„ìŠ¤**:
â€¢ **ì •í™•í•œ ë²•ì  ì •ì˜** ì œê³µ
â€¢ **ì‰¬ìš´ ì„¤ëª…** ë° ì˜ˆì‹œ
â€¢ **ê´€ë ¨ ìš©ì–´** ì—°ê²°
â€¢ **ì‹¤ë¬´ì—ì„œì˜ ì ìš©** ì‚¬ë¡€

**ğŸ“‹ í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥**:
â€¢ âš–ï¸ **íŒë¡€ ê²€ìƒ‰** âœ… (ì™„ë£Œ)

**ğŸ¯ ì˜ˆì‹œ ì§ˆë¬¸**:
â€¢ "ëŒ€ë²•ì› 2019ë‹¤12345 íŒë¡€ ë‚´ìš©ì€?"
â€¢ "ì‹ í˜¸ìœ„ë°˜ ê´€ë ¨ íŒë¡€ ê²€ìƒ‰"

**ë¹ ë¥¸ ì‹œì¼ ë‚´ì— ìš©ì–´ ì„¤ëª… ê¸°ëŠ¥ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!** ğŸ“š"""
    
    def _process_general_placeholder(self, user_input: str) -> str:
        """ì¼ë°˜ ì§ˆë¬¸ í”Œë ˆì´ìŠ¤í™€ë” - ê°œì„ ëœ í¬ë§·"""
        return f"""ğŸ‘‹ **ë…¸ëŠ ìƒë‹´ ì±—ë´‡**

ì•ˆë…•í•˜ì„¸ìš”! êµí†µì‚¬ê³  ê³¼ì‹¤ë¹„ìœ¨ ìƒë‹´ ì±—ë´‡ **ë…¸ëŠ**ì…ë‹ˆë‹¤! ğŸš—

**ğŸ¯ í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥**:

**âœ… íŒë¡€ ê²€ìƒ‰** (ì™„ë£Œ)
â€¢ "ëŒ€ë²•ì› 92ë„2077 íŒë¡€ ë‚´ìš©ì€?"
â€¢ "êµì°¨ë¡œ ì¢ŒíšŒì „ ì‚¬ê³  íŒë¡€ ê²€ìƒ‰"
â€¢ "ì‹ í˜¸ìœ„ë°˜ ê´€ë ¨ íŒë¡€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

**ğŸ”§ ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥**:
â€¢ ğŸš— **êµí†µì‚¬ê³  ê³¼ì‹¤ë¹„ìœ¨ ë¶„ì„**
â€¢ ğŸ“– **êµí†µì‚¬ê³  ìš©ì–´ ì„¤ëª…**

**ğŸ’¡ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´**:
â€¢ êµ¬ì²´ì ì¸ ì‚¬ê±´ë²ˆí˜¸ë‚˜ ì‚¬ê³  ìƒí™©ì„ ì•Œë ¤ì£¼ì„¸ìš”
â€¢ ê¶ê¸ˆí•œ íŒë¡€ë‚˜ ë²•ë¥  ì¡°ë¬¸ì„ ë§ì”€í•´ì£¼ì„¸ìš”

**ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?** ğŸ˜Š"""
    
    def _generate_error_response(self, user_input: str, error_msg: str) -> str:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‘ë‹µ - ê°œì„ ëœ í¬ë§·"""
        return f"""âŒ **ì‹œìŠ¤í…œ ì¼ì‹œ ì˜¤ë¥˜**

**ğŸ” ìš”ì²­ ë‚´ìš©**: "{user_input}"

**âš ï¸ ì˜¤ë¥˜ ìƒí™©**
ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ğŸ’¡ í•´ê²° ë°©ë²•**:
â€¢ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
â€¢ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”
â€¢ êµ¬ì²´ì ì¸ ì‚¬ê±´ë²ˆí˜¸ë‚˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”

**ğŸ¯ ì¶”ì²œ ì§ˆë¬¸ ë°©ì‹**:
â€¢ "ëŒ€ë²•ì› [ì‚¬ê±´ë²ˆí˜¸] íŒë¡€"
â€¢ "[ì‚¬ê³ ìœ í˜•] ê´€ë ¨ íŒë¡€"
â€¢ "[ë²•ì›ëª…] êµí†µì‚¬ê³  íŒë¡€"

**ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!** ğŸ™

*ê¸°ìˆ  ì •ë³´: {error_msg}*"""
    
    def get_classification_stats(self) -> Dict[str, Any]:
        """
        ë¶„ë¥˜ê¸° í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        return {
            'model_id': self.model_id,
            'valid_categories': list(self.VALID_CATEGORIES),
            'fallback_keywords_count': {
                category: len(keywords) 
                for category, keywords in self.FALLBACK_KEYWORDS.items()
            },
            'implemented_functions': ['classify_query', 'process_precedent', 'process_law'],
            'upcoming_functions': ['process_accident', 'process_term'],
            'status': 'partial_implementation'
        }


# ì „ì—­ ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_classifier_instance = None

def get_classifier() -> TrafficAccidentClassifier:
    """
    ë¶„ë¥˜ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        TrafficAccidentClassifier: ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤
    """
    global _classifier_instance
    
    if _classifier_instance is None:
        _classifier_instance = TrafficAccidentClassifier()
    
    return _classifier_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
def classify_user_query(user_input: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ë¥˜ í¸ì˜ í•¨ìˆ˜
    
    Args:
        user_input (str): ì‚¬ìš©ì ì…ë ¥
        
    Returns:
        str: ë¶„ë¥˜ ê²°ê³¼
    """
    classifier = get_classifier()
    return classifier.classify_query(user_input)


def process_user_query(user_input: str) -> tuple:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ í†µí•© ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜
    
    Args:
        user_input (str): ì‚¬ìš©ì ì…ë ¥
        
    Returns:
        tuple: (category, response)
    """
    classifier = get_classifier()
    return classifier.process_user_query(user_input)
